{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import types, functions as F,SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "23/04/24 16:16:50 WARN Utils: Your hostname, MSI resolves to a loopback address: 127.0.1.1; using 192.168.37.32 instead (on interface eth0)\n",
      "23/04/24 16:16:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/04/24 16:16:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").appName('test').getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Spark Udf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cubed(num):\n",
    "    return num*num*num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.cubed(num)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# registering the udf\n",
    "spark.udf.register(\"cubed\", cubed, types.LongType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.range(1,9).createOrReplaceTempView('cube_view')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|cubed_id|\n",
      "+---+--------+\n",
      "|  1|       1|\n",
      "|  2|       8|\n",
      "|  3|      27|\n",
      "|  4|      64|\n",
      "|  5|     125|\n",
      "|  6|     216|\n",
      "|  7|     343|\n",
      "|  8|     512|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select id, cubed(id) as cubed_id from cube_view').show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Pandas udf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "cubed_pd_udf = pandas_udf(cubed, types.LongType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| id|cubed(id)|\n",
      "+---+---------+\n",
      "|  1|        1|\n",
      "|  2|        8|\n",
      "|  3|       27|\n",
      "|  4|       64|\n",
      "|  5|      125|\n",
      "|  6|      216|\n",
      "|  7|      343|\n",
      "|  8|      512|\n",
      "|  9|      729|\n",
      "| 10|     1000|\n",
      "| 11|     1331|\n",
      "| 12|     1728|\n",
      "| 13|     2197|\n",
      "| 14|     2744|\n",
      "| 15|     3375|\n",
      "| 16|     4096|\n",
      "| 17|     4913|\n",
      "| 18|     5832|\n",
      "| 19|     6859|\n",
      "| 20|     8000|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.range(1,50)\n",
    "df.select(\"id\", cubed_pd_udf(F.col(\"id\"))).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Note`\n",
    "\n",
    "use spark sbin start-thriftserver, stop-thriftserver , to connect external tools like tableau, dbeaver ,etc to spark tables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `High order functions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.text('/home/snehil/Desktop/data-engineering-resources/4_spark/spark_book/data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select('value',F.explode(F.split(df.value, ',').alias('exploded')))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### transform\n",
    "\n",
    "* The PySpark sql.functions.transform() is used to apply the transformation on a column of type Array. This function applies the specified transformation on every element of the array and returns an object of ArrayType."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+\n",
      "|            value|            splitted|\n",
      "+-----------------+--------------------+\n",
      "|State,Color,Count|[State, Color, Co...|\n",
      "|        TX,Red,20|       [TX, Red, 20]|\n",
      "|       NV,Blue,66|      [NV, Blue, 66]|\n",
      "|       CO,Blue,79|      [CO, Blue, 79]|\n",
      "|       OR,Blue,71|      [OR, Blue, 71]|\n",
      "|     WA,Yellow,93|    [WA, Yellow, 93]|\n",
      "|       WY,Blue,16|      [WY, Blue, 16]|\n",
      "|     CA,Yellow,53|    [CA, Yellow, 53]|\n",
      "|      WA,Green,60|     [WA, Green, 60]|\n",
      "|      OR,Green,71|     [OR, Green, 71]|\n",
      "|      TX,Green,68|     [TX, Green, 68]|\n",
      "|      NV,Green,59|     [NV, Green, 59]|\n",
      "|      AZ,Brown,95|     [AZ, Brown, 95]|\n",
      "|     WA,Yellow,20|    [WA, Yellow, 20]|\n",
      "|       AZ,Blue,75|      [AZ, Blue, 75]|\n",
      "|      OR,Brown,72|     [OR, Brown, 72]|\n",
      "|        NV,Red,98|       [NV, Red, 98]|\n",
      "|     WY,Orange,45|    [WY, Orange, 45]|\n",
      "|       CO,Blue,52|      [CO, Blue, 52]|\n",
      "|      TX,Brown,94|     [TX, Brown, 94]|\n",
      "+-----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.text('/home/snehil/Desktop/data-engineering-resources/4_spark/spark_book/data/')\n",
    "df = df.select('value',F.split(df.value, ',').alias('splitted'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+---------------------+\n",
      "|splitted             |splitted_UPPER       |\n",
      "+---------------------+---------------------+\n",
      "|[State, Color, Count]|[STATE, COLOR, COUNT]|\n",
      "|[TX, Red, 20]        |[TX, RED, 20]        |\n",
      "|[NV, Blue, 66]       |[NV, BLUE, 66]       |\n",
      "|[CO, Blue, 79]       |[CO, BLUE, 79]       |\n",
      "|[OR, Blue, 71]       |[OR, BLUE, 71]       |\n",
      "|[WA, Yellow, 93]     |[WA, YELLOW, 93]     |\n",
      "|[WY, Blue, 16]       |[WY, BLUE, 16]       |\n",
      "|[CA, Yellow, 53]     |[CA, YELLOW, 53]     |\n",
      "|[WA, Green, 60]      |[WA, GREEN, 60]      |\n",
      "|[OR, Green, 71]      |[OR, GREEN, 71]      |\n",
      "|[TX, Green, 68]      |[TX, GREEN, 68]      |\n",
      "|[NV, Green, 59]      |[NV, GREEN, 59]      |\n",
      "|[AZ, Brown, 95]      |[AZ, BROWN, 95]      |\n",
      "|[WA, Yellow, 20]     |[WA, YELLOW, 20]     |\n",
      "|[AZ, Blue, 75]       |[AZ, BLUE, 75]       |\n",
      "|[OR, Brown, 72]      |[OR, BROWN, 72]      |\n",
      "|[NV, Red, 98]        |[NV, RED, 98]        |\n",
      "|[WY, Orange, 45]     |[WY, ORANGE, 45]     |\n",
      "|[CO, Blue, 52]       |[CO, BLUE, 52]       |\n",
      "|[TX, Brown, 94]      |[TX, BROWN, 94]      |\n",
      "+---------------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.splitted,F.transform('splitted', lambda x:F.upper(x)).alias('splitted_UPPER')).show(truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### filter\n",
    "\n",
    "* The filter() function produces an array consisting of only the elements of the input\n",
    "array for which the Boolean function is true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|            splitted|               color|\n",
      "+--------------------+--------------------+\n",
      "|[State, Color, Co...|[State, Color, Co...|\n",
      "|       [TX, Red, 20]|               [Red]|\n",
      "|      [NV, Blue, 66]|              [Blue]|\n",
      "|      [CO, Blue, 79]|              [Blue]|\n",
      "|      [OR, Blue, 71]|              [Blue]|\n",
      "|    [WA, Yellow, 93]|            [Yellow]|\n",
      "|      [WY, Blue, 16]|              [Blue]|\n",
      "|    [CA, Yellow, 53]|            [Yellow]|\n",
      "|     [WA, Green, 60]|             [Green]|\n",
      "|     [OR, Green, 71]|             [Green]|\n",
      "|     [TX, Green, 68]|             [Green]|\n",
      "|     [NV, Green, 59]|             [Green]|\n",
      "|     [AZ, Brown, 95]|             [Brown]|\n",
      "|    [WA, Yellow, 20]|            [Yellow]|\n",
      "|      [AZ, Blue, 75]|              [Blue]|\n",
      "|     [OR, Brown, 72]|             [Brown]|\n",
      "|       [NV, Red, 98]|               [Red]|\n",
      "|    [WY, Orange, 45]|            [Orange]|\n",
      "|      [CO, Blue, 52]|              [Blue]|\n",
      "|     [TX, Brown, 94]|             [Brown]|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.splitted, F.filter('splitted', lambda x:F.length(x)>2).alias('color')).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### exists\n",
    "\n",
    "* The exists() function returns true if the Boolean function holds for any element in\n",
    "the input array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|            splitted|state is TX|\n",
      "+--------------------+-----------+\n",
      "|[State, Color, Co...|      false|\n",
      "|       [TX, Red, 20]|       true|\n",
      "|      [NV, Blue, 66]|      false|\n",
      "|      [CO, Blue, 79]|      false|\n",
      "|      [OR, Blue, 71]|      false|\n",
      "|    [WA, Yellow, 93]|      false|\n",
      "|      [WY, Blue, 16]|      false|\n",
      "|    [CA, Yellow, 53]|      false|\n",
      "|     [WA, Green, 60]|      false|\n",
      "|     [OR, Green, 71]|      false|\n",
      "|     [TX, Green, 68]|       true|\n",
      "|     [NV, Green, 59]|      false|\n",
      "|     [AZ, Brown, 95]|      false|\n",
      "|    [WA, Yellow, 20]|      false|\n",
      "|      [AZ, Blue, 75]|      false|\n",
      "|     [OR, Brown, 72]|      false|\n",
      "|       [NV, Red, 98]|      false|\n",
      "|    [WY, Orange, 45]|      false|\n",
      "|      [CO, Blue, 52]|      false|\n",
      "|     [TX, Brown, 94]|       true|\n",
      "+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.splitted,F.exists('splitted',lambda x:x=='TX').alias('state is TX')).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Also available functions include join , union ,windowing ,etc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
