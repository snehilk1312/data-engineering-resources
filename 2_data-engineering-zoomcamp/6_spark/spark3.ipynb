{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession,types\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "23/04/16 04:04:49 WARN Utils: Your hostname, MSI resolves to a loopback address: 127.0.1.1; using 172.24.150.144 instead (on interface eth0)\n",
      "23/04/16 04:04:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/04/16 04:04:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").appName('test').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green_pd = pd.read_csv('data/raw/green/2020/01/green_tripdata_2020_01.csv.gz',nrows=100)\n",
    "df_yellow_pd = pd.read_csv('data/raw/yellow/2020/01/yellow_tripdata_2020_01.csv.gz',nrows=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n",
      "|VendorID|lpep_pickup_datetime|lpep_dropoff_datetime|store_and_fwd_flag|RatecodeID|PULocationID|DOLocationID|passenger_count|trip_distance|fare_amount|extra|mta_tax|tip_amount|tolls_amount|ehail_fee|improvement_surcharge|total_amount|payment_type|trip_type|congestion_surcharge|\n",
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n",
      "|       2| 2019-12-18 15:52:30|  2019-12-18 15:54:39|                 N|         1|         264|         264|              5|          0.0|        3.5|  0.5|    0.5|      0.01|         0.0|      NaN|                  0.3|        4.81|           1|        1|                 0.0|\n",
      "|       2| 2020-01-01 00:45:58|  2020-01-01 00:56:39|                 N|         5|          66|          65|              2|         1.28|       20.0|  0.0|    0.0|      4.06|         0.0|      NaN|                  0.3|       24.36|           1|        2|                 0.0|\n",
      "|       2| 2020-01-01 00:41:38|  2020-01-01 00:52:49|                 N|         1|         181|         228|              1|         2.47|       10.5|  0.5|    0.5|      3.54|         0.0|      NaN|                  0.3|       15.34|           1|        1|                 0.0|\n",
      "|       1| 2020-01-01 00:52:46|  2020-01-01 01:14:21|                 N|         1|         129|         263|              2|          6.3|       21.0| 3.25|    0.5|       0.0|         0.0|      NaN|                  0.3|       25.05|           2|        1|                2.75|\n",
      "|       1| 2020-01-01 00:19:57|  2020-01-01 00:30:56|                 N|         1|         210|         150|              1|          2.3|       10.0|  0.5|    0.5|       0.0|         0.0|      NaN|                  0.3|        11.3|           1|        1|                 0.0|\n",
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(df_green_pd).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('VendorID', LongType(), True), StructField('lpep_pickup_datetime', StringType(), True), StructField('lpep_dropoff_datetime', StringType(), True), StructField('store_and_fwd_flag', StringType(), True), StructField('RatecodeID', LongType(), True), StructField('PULocationID', LongType(), True), StructField('DOLocationID', LongType(), True), StructField('passenger_count', LongType(), True), StructField('trip_distance', DoubleType(), True), StructField('fare_amount', DoubleType(), True), StructField('extra', DoubleType(), True), StructField('mta_tax', DoubleType(), True), StructField('tip_amount', DoubleType(), True), StructField('tolls_amount', DoubleType(), True), StructField('ehail_fee', DoubleType(), True), StructField('improvement_surcharge', DoubleType(), True), StructField('total_amount', DoubleType(), True), StructField('payment_type', LongType(), True), StructField('trip_type', LongType(), True), StructField('congestion_surcharge', DoubleType(), True)])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame(df_green_pd).schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('VendorID', LongType(), True), StructField('tpep_pickup_datetime', StringType(), True), StructField('tpep_dropoff_datetime', StringType(), True), StructField('passenger_count', LongType(), True), StructField('trip_distance', DoubleType(), True), StructField('RatecodeID', LongType(), True), StructField('store_and_fwd_flag', StringType(), True), StructField('PULocationID', LongType(), True), StructField('DOLocationID', LongType(), True), StructField('payment_type', LongType(), True), StructField('fare_amount', DoubleType(), True), StructField('extra', DoubleType(), True), StructField('mta_tax', DoubleType(), True), StructField('tip_amount', DoubleType(), True), StructField('tolls_amount', DoubleType(), True), StructField('improvement_surcharge', DoubleType(), True), StructField('total_amount', DoubleType(), True), StructField('congestion_surcharge', DoubleType(), True)])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame(df_yellow_pd).schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_green = types.StructType([\n",
    "    types.StructField('VendorID',types.IntegerType(), True), \n",
    "    types.StructField('lpep_pickup_datetime',types.TimestampType(), True), \n",
    "    types.StructField('lpep_dropoff_datetime',types.TimestampType(), True), \n",
    "    types.StructField('store_and_fwd_flag',types.StringType(), True), \n",
    "    types.StructField('RatecodeID',types.IntegerType(), True), \n",
    "    types.StructField('PULocationID',types.IntegerType(), True), \n",
    "    types.StructField('DOLocationID',types.IntegerType(), True), \n",
    "    types.StructField('passenger_count',types.IntegerType(), True), \n",
    "    types.StructField('trip_distance',types.DoubleType(), True), \n",
    "    types.StructField('fare_amount',types.DoubleType(), True), \n",
    "    types.StructField('extra',types.DoubleType(), True), \n",
    "    types.StructField('mta_tax',types.DoubleType(), True), \n",
    "    types.StructField('tip_amount',types.DoubleType(), True), \n",
    "    types.StructField('tolls_amount',types.DoubleType(), True), \n",
    "    types.StructField('ehail_fee',types.DoubleType(), True), \n",
    "    types.StructField('improvement_surcharge',types.DoubleType(), True), \n",
    "    types.StructField('total_amount',types.DoubleType(), True), \n",
    "    types.StructField('payment_type',types.IntegerType(), True), \n",
    "    types.StructField('trip_type',types.IntegerType(), True), \n",
    "    types.StructField('congestion_surcharge',types.DoubleType(), True)\n",
    "])\n",
    "schema_yellow = types.StructType([\n",
    "    types.StructField('VendorID',types.IntegerType(), True), \n",
    "    types.StructField('tpep_pickup_datetime',types.TimestampType(), True), \n",
    "    types.StructField('tpep_dropoff_datetime',types.TimestampType(), True), \n",
    "    types.StructField('passenger_count',types.IntegerType(), True), \n",
    "    types.StructField('trip_distance',types.DoubleType(), True), \n",
    "    types.StructField('RatecodeID',types.IntegerType(), True), \n",
    "    types.StructField('store_and_fwd_flag',types.StringType(), True), \n",
    "    types.StructField('PULocationID',types.IntegerType(), True), \n",
    "    types.StructField('DOLocationID',types.IntegerType(), True), \n",
    "    types.StructField('payment_type',types.IntegerType(), True), \n",
    "    types.StructField('fare_amount',types.DoubleType(), True), \n",
    "    types.StructField('extra',types.DoubleType(), True), \n",
    "    types.StructField('mta_tax',types.DoubleType(), True), \n",
    "    types.StructField('tip_amount',types.DoubleType(), True), \n",
    "    types.StructField('tolls_amount',types.DoubleType(), True), \n",
    "    types.StructField('improvement_surcharge',types.DoubleType(), True), \n",
    "    types.StructField('total_amount',types.DoubleType(), True), \n",
    "    types.StructField('congestion_surcharge',types.DoubleType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing the data for month 2020/01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing the data for month 2020/02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing the data for month 2020/03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing the data for month 2020/04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing the data for month 2020/05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing the data for month 2020/06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing the data for month 2020/07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing the data for month 2020/08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing the data for month 2020/09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing the data for month 2020/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing the data for month 2020/11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing the data for month 2020/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "year = 2020\n",
    "color = 'yellow'\n",
    "for month in range(1,13):\n",
    "    print(f'processing the data for month {year}/{month:02}')\n",
    "\n",
    "    input_path = f'data/raw/{color}/{year}/{month:02}/'\n",
    "    output_path = f'data/pq/{color}/{year}/{month:02}/'\n",
    "\n",
    "    try:\n",
    "        if color=='yellow':\n",
    "            df = spark.read.option(\"header\",\"true\").schema(schema_yellow).csv(f\"{input_path}\")\n",
    "        if color=='green':\n",
    "            df = spark.read.option(\"header\",\"true\").schema(schema_green).csv(f\"{input_path}\")\n",
    "    except pyspark.sql.utils.AnalysisException:\n",
    "        print(f'{input_path} don\\'t exist !')\n",
    "    df.repartition(4).write.mode('overwrite').parquet(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
